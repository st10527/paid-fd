# PAID-FD: Privacy-Aware Incentive-Driven Federated Distillation

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A simulation framework for **Privacy-Aware Incentive-Driven Federated Distillation** with Stackelberg game-based incentive mechanism.

> ğŸ“ **Paper**: Submitted to IEEE Transactions on Mobile Computing (TMC)

## ğŸŒŸ Highlights

- **Stackelberg Game Mechanism**: One-shot broadcast pricing with optimal device response
- **Three-Dimensional Heterogeneity**: Communication, privacy sensitivity, and computation costs
- **Adaptive Privacy**: Device-specific Îµ allocation via game equilibrium  
- **Energy Efficient**: ~99% energy savings compared to traditional FL
- **Cross-Domain Distillation**: CIFAR-100 (private) + STL-10 (public)

## ğŸ“ Project Structure

```
paid_fd/
â”œâ”€â”€ config/                     # YAML configurations
â”‚   â”œâ”€â”€ default.yaml            # Default settings
â”‚   â”œâ”€â”€ experiments/            # Experiment configs
â”‚   â””â”€â”€ methods/                # Method configs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                   # Data loading & partitioning
â”‚   â”‚   â”œâ”€â”€ datasets.py         # CIFAR-100, STL-10, Synthetic
â”‚   â”‚   â””â”€â”€ partition.py        # Dirichlet Non-IID
â”‚   â”œâ”€â”€ devices/                # Device simulation
â”‚   â”‚   â”œâ”€â”€ heterogeneity.py    # 3D heterogeneity model
â”‚   â”‚   â””â”€â”€ energy.py           # Energy consumption
â”‚   â”œâ”€â”€ game/                   # Game theory
â”‚   â”‚   â”œâ”€â”€ stackelberg.py      # Algorithms 1 & 2
â”‚   â”‚   â””â”€â”€ utility.py          # Quality functions
â”‚   â”œâ”€â”€ privacy/                # Privacy mechanisms
â”‚   â”‚   â””â”€â”€ ldp.py              # Laplace/Gaussian DP
â”‚   â”œâ”€â”€ models/                 # Neural networks
â”‚   â”‚   â”œâ”€â”€ resnet.py           # ResNet-18/34
â”‚   â”‚   â””â”€â”€ cnn.py              # Lightweight CNNs
â”‚   â”œâ”€â”€ methods/                # FL methods
â”‚   â”‚   â”œâ”€â”€ paid_fd.py          # Our method
â”‚   â”‚   â””â”€â”€ fixed_eps.py        # Ablation baseline
â”‚   â””â”€â”€ utils/                  # Utilities
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_experiment.py       # Unified runner
â”œâ”€â”€ results/                    # Output directory
â”œâ”€â”€ scripts/                    # Helper scripts
â””â”€â”€ tests/                      # Unit tests
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/paid-fd.git
cd paid-fd

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run Experiments

```bash
# Quick test with synthetic data (no download, CPU friendly)
python experiments/run_experiment.py \
    --config exp2_convergence \
    --method PAID-FD \
    --synthetic \
    --rounds 10

# Full experiment with real data (requires GPU)
python experiments/run_experiment.py \
    --config exp2_convergence \
    --method PAID-FD

# Run multiple methods
python experiments/run_experiment.py \
    --config exp2_convergence \
    --method all

# Specify device
python experiments/run_experiment.py \
    --config exp2_convergence \
    --method PAID-FD \
    --device cuda:0
```

### Test Core Components (No PyTorch Required)

```python
from src.devices.heterogeneity import HeterogeneityGenerator
from src.game.stackelberg import StackelbergSolver

# Generate 50 heterogeneous devices
gen = HeterogeneityGenerator(n_devices=50, seed=42)
devices = gen.generate()

# Solve Stackelberg game
solver = StackelbergSolver(gamma=10.0)
result = solver.solve(devices)

print(f"Optimal price: {result['price']:.4f}")
print(f"Participation: {result['participation_rate']:.0%}")
print(f"Avg Îµ*: {result['avg_eps']:.3f}")
```

## âš™ï¸ Configuration

Edit `config/default.yaml`:

```yaml
system:
  n_devices: 50
  seed: 42

data:
  partition:
    method: dirichlet
    alpha: 0.5  # Non-IID level

paid_fd:
  gamma: 10.0      # Server valuation
  clip_bound: 5.0  # LDP clipping

training:
  n_rounds: 200
  local_epochs: 1
  distill_epochs: 5
```

## ğŸ“Š Experiments

| Exp | Description | Config |
|-----|-------------|--------|
| 1 | Algorithm Efficiency | `exp1_efficiency.yaml` |
| 2 | Convergence & Accuracy | `exp2_convergence.yaml` |
| 3 | Privacy-Accuracy Tradeoff | `exp3_privacy.yaml` |
| 4 | Energy Analysis | `exp4_energy.yaml` |
| 5 | Heterogeneity Impact | `exp5_heterogeneity.yaml` |
| 6 | Incentive Analysis | `exp6_incentive.yaml` |
| 7 | Scalability | `exp7_scalability.yaml` |

## ğŸ“ˆ Results

Results are saved to `results/experiments/{exp_name}/`:

```python
from src.utils.results import ResultManager

manager = ResultManager()

# List all results
files = manager.list_results("exp2_convergence")

# Compare methods
comparison = manager.compare_results("exp2_convergence", metric="final_accuracy")
print(comparison)
```

## ğŸ”§ Methods

| Method | Description | Type | Status |
|--------|-------------|------|--------|
| **PAID-FD** | Stackelberg game + adaptive Îµ (ours) | FD + LDP | âœ… |
| **Fixed-Îµ** | Fixed privacy budget ablation | FD + LDP | âœ… |
| **FedMD** | FD baseline, no privacy (Li & Wang, NeurIPS 2019) | FD | âœ… |
| **FedAvg** | Parameter averaging (McMahan et al., 2017) | Param-Avg | âœ… |
| **CSRA** | Reverse auction DPFL (Yang et al., TIFS 2024) | Param-Avg + DP | âœ… |
| **FedGMKD** | GMM prototype KD + DAT (Zhang et al., 2024) | Prototype | âœ… |

## ğŸ’» Hardware Requirements

| Task | Minimum | Recommended |
|------|---------|-------------|
| Development | CPU | GPU (any) |
| Single Experiment | GTX 1080 | RTX 3090 |
| Full Experiments | RTX 3090 | A100 / Multi-GPU |

**Memory**: ~3GB VRAM for ResNet-18 + CIFAR-100

## ğŸ“„ Citation

```bibtex
@article{paid_fd_2026,
  title={Privacy-Aware Incentive-Driven Federated Distillation 
         for Heterogeneous Edge Networks},
  author={...},
  journal={IEEE Transactions on Mobile Computing},
  year={2026}
}
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- CIFAR-100 and STL-10 datasets
- PyTorch team
- Federated learning community
